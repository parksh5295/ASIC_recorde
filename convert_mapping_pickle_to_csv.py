"""
Convert mapping pickle file to CSV format.

This script converts category_mapping pickle files (generated by choose_heterogeneous_method)
to CSV format (mapped_info.csv) that can be used by other scripts.

Usage:
    python convert_mapping_pickle_to_csv.py --file_type NSL-KDD --file_number 1 --heterogeneous_method Interval_inverse
    python convert_mapping_pickle_to_csv.py --file_type NSL-KDD --file_number 1 --heterogeneous_method Interval_inverse --output_dir ../Dataset_Paral/signature
"""

import argparse
import pickle
import os
import pandas as pd
import logging

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


def load_mapping_from_pickle(pickle_path):
    """
    Load category_mapping from pickle file.
    
    Args:
        pickle_path (str): Path to the pickle file
        
    Returns:
        dict: category_mapping dictionary with 'interval', 'categorical', 'binary' keys
    """
    if not os.path.exists(pickle_path):
        logger.error(f"Pickle file not found: {pickle_path}")
        return None
    
    try:
        with open(pickle_path, 'rb') as f:
            category_mapping = pickle.load(f)
        logger.info(f"Successfully loaded mapping from: {pickle_path}")
        return category_mapping
    except Exception as e:
        logger.error(f"Failed to load pickle file {pickle_path}: {e}")
        return None


def convert_category_mapping_to_csv(category_mapping, output_path):
    """
    Convert category_mapping dictionary to CSV format.
    
    The CSV format has:
    - Columns: feature names (from interval, categorical, binary mappings)
    - Rows: mapping rules (e.g., "(0.5, 1.5]=1" for intervals, "value=group" for categorical)
    
    Args:
        category_mapping (dict): Dictionary with 'interval', 'categorical', 'binary' keys
        output_path (str): Path where the CSV file will be saved
        
    Returns:
        bool: True if successful, False otherwise
    """
    if not category_mapping:
        logger.error("category_mapping is None or empty")
        return False
    
    # Prepare DataFrame to store all mappings
    all_mappings = {}
    
    # --- 1. Process Interval Mapping ---
    interval_mapping = category_mapping.get('interval')
    if isinstance(interval_mapping, pd.DataFrame) and not interval_mapping.empty:
        logger.info(f"Processing {len(interval_mapping.columns)} interval features...")
        for col in interval_mapping.columns:
            # Get all non-null mapping rules for this column
            rules = interval_mapping[col].dropna().tolist()
            if rules:
                # Convert to string format if needed
                rules_str = [str(rule) if not isinstance(rule, str) else rule for rule in rules]
                all_mappings[col] = rules_str
    
    # --- 2. Process Categorical Mapping ---
    categorical_mapping = category_mapping.get('categorical')
    if categorical_mapping is not None:
        if isinstance(categorical_mapping, dict):
            logger.info(f"Processing {len(categorical_mapping)} categorical features...")
            for feature, mapping_dict in categorical_mapping.items():
                if isinstance(mapping_dict, dict):
                    # Convert dict to "value=group" format
                    rules = [f"{k}={v}" for k, v in mapping_dict.items()]
                    all_mappings[feature] = rules
                elif isinstance(mapping_dict, pd.Series):
                    # Already in Series format, convert to list
                    rules = mapping_dict.dropna().tolist()
                    all_mappings[feature] = [str(r) if not isinstance(r, str) else r for r in rules]
        elif isinstance(categorical_mapping, pd.DataFrame) and not categorical_mapping.empty:
            logger.info(f"Processing categorical DataFrame with {len(categorical_mapping.columns)} features...")
            for col in categorical_mapping.columns:
                rules = categorical_mapping[col].dropna().tolist()
                if rules:
                    all_mappings[col] = [str(r) if not isinstance(r, str) else r for r in rules]
    
    # --- 3. Process Binary Mapping ---
    binary_mapping = category_mapping.get('binary')
    if binary_mapping is not None:
        if isinstance(binary_mapping, dict):
            logger.info(f"Processing {len(binary_mapping)} binary features...")
            for feature, mapping_dict in binary_mapping.items():
                if isinstance(mapping_dict, dict):
                    rules = [f"{k}={v}" for k, v in mapping_dict.items()]
                    all_mappings[feature] = rules
                elif isinstance(mapping_dict, pd.Series):
                    rules = mapping_dict.dropna().tolist()
                    all_mappings[feature] = [str(r) if not isinstance(r, str) else r for r in rules]
        elif isinstance(binary_mapping, pd.DataFrame) and not binary_mapping.empty:
            logger.info(f"Processing binary DataFrame with {len(binary_mapping.columns)} features...")
            for col in binary_mapping.columns:
                rules = binary_mapping[col].dropna().tolist()
                if rules:
                    all_mappings[col] = [str(r) if not isinstance(r, str) else r for r in rules]
    
    if not all_mappings:
        logger.warning("No mappings found in category_mapping. CSV will be empty.")
        return False
    
    # Create DataFrame from all mappings
    # Find the maximum number of rules across all features to determine DataFrame length
    max_rules = max(len(rules) for rules in all_mappings.values()) if all_mappings else 0
    
    # Create DataFrame with all features as columns
    mapping_df = pd.DataFrame(index=range(max_rules))
    for feature, rules in all_mappings.items():
        # Pad rules with empty strings if needed
        padded_rules = rules + [''] * (max_rules - len(rules))
        mapping_df[feature] = padded_rules
    
    # Ensure output directory exists
    output_dir = os.path.dirname(output_path)
    if output_dir and not os.path.exists(output_dir):
        os.makedirs(output_dir, exist_ok=True)
        logger.info(f"Created output directory: {output_dir}")
    
    # Save to CSV
    try:
        mapping_df.to_csv(output_path, index=False)
        logger.info(f"Successfully saved mapping CSV to: {output_path}")
        logger.info(f"CSV shape: {mapping_df.shape} (rows: {max_rules}, columns: {len(all_mappings)})")
        return True
    except Exception as e:
        logger.error(f"Failed to save CSV to {output_path}: {e}")
        return False


def main():
    parser = argparse.ArgumentParser(
        description="Convert mapping pickle file to CSV format",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Convert with default paths
  python convert_mapping_pickle_to_csv.py --file_type NSL-KDD --file_number 1 --heterogeneous_method Interval_inverse
  
  # Specify custom output directory
  python convert_mapping_pickle_to_csv.py --file_type NSL-KDD --file_number 1 --heterogeneous_method Interval_inverse --output_dir ../Dataset_Paral/signature
  
  # Specify custom pickle input path
  python convert_mapping_pickle_to_csv.py --pickle_path ../Dataset_ex/mapping_info/NSL-KDD_1_Interval_inverse_mapping.pkl --output_path ../Dataset_Paral/signature/NSL-KDD/NSL-KDD_1_mapped_info.csv
        """
    )
    
    parser.add_argument('--file_type', type=str, help="Dataset type (e.g., NSL-KDD, MiraiBotnet)")
    parser.add_argument('--file_number', type=int, default=1,
                        help="File number (default: 1)")
    parser.add_argument('--heterogeneous_method', type=str, default='Interval_inverse',
                        help="Heterogeneous method name (default: Interval_inverse)")
    parser.add_argument('--pickle_dir', type=str, default='../Dataset_ex/mapping_info',
                        help="Directory containing pickle files (default: ../Dataset_ex/mapping_info)")
    parser.add_argument('--output_dir', type=str, default='../Dataset_Paral/signature',
                        help="Output directory for CSV file (default: ../Dataset_Paral/signature)")
    parser.add_argument('--pickle_path', type=str, default=None,
                        help="Direct path to pickle file (overrides file_type/file_number)")
    parser.add_argument('--output_path', type=str, default=None,
                        help="Direct path for output CSV (overrides file_type/file_number/output_dir)")
    
    args = parser.parse_args()
    
    # Determine pickle path
    if args.pickle_path:
        pickle_path = args.pickle_path
    elif args.file_type:
        # If only file_type is provided, use file_number=1 as default
        pickle_filename = f"{args.file_type}_{args.file_number}_{args.heterogeneous_method}_mapping.pkl"
        pickle_path = os.path.join(args.pickle_dir, pickle_filename)
    else:
        logger.error("Either --pickle_path or --file_type must be provided")
        return
    
    # Determine output path
    if args.output_path:
        output_path = args.output_path
    elif args.file_type:
        # If only file_type is provided, use file_number=1 as default
        csv_filename = f"{args.file_type}_{args.file_number}_mapped_info.csv"
        output_dir_full = os.path.join(args.output_dir, args.file_type)
        output_path = os.path.join(output_dir_full, csv_filename)
    else:
        logger.error("Either --output_path or --file_type must be provided")
        return
    
    logger.info("=" * 80)
    logger.info("Mapping Pickle to CSV Converter")
    logger.info("=" * 80)
    logger.info(f"Input pickle: {pickle_path}")
    logger.info(f"Output CSV: {output_path}")
    logger.info("=" * 80)
    
    # Load mapping from pickle
    category_mapping = load_mapping_from_pickle(pickle_path)
    if category_mapping is None:
        return
    
    # Log mapping structure
    logger.info("\nMapping structure:")
    if 'interval' in category_mapping:
        interval = category_mapping['interval']
        if isinstance(interval, pd.DataFrame):
            logger.info(f"  Interval: DataFrame with {len(interval.columns)} features")
        else:
            logger.info(f"  Interval: {type(interval)}")
    if 'categorical' in category_mapping:
        cat = category_mapping['categorical']
        if isinstance(cat, dict):
            logger.info(f"  Categorical: dict with {len(cat)} features")
        elif isinstance(cat, pd.DataFrame):
            logger.info(f"  Categorical: DataFrame with {len(cat.columns)} features")
        else:
            logger.info(f"  Categorical: {type(cat)}")
    if 'binary' in category_mapping:
        bin_map = category_mapping['binary']
        if isinstance(bin_map, dict):
            logger.info(f"  Binary: dict with {len(bin_map)} features")
        elif isinstance(bin_map, pd.DataFrame):
            logger.info(f"  Binary: DataFrame with {len(bin_map.columns)} features")
        else:
            logger.info(f"  Binary: {type(bin_map)}")
    
    # Convert to CSV
    success = convert_category_mapping_to_csv(category_mapping, output_path)
    
    if success:
        logger.info("\n" + "=" * 80)
        logger.info("Conversion completed successfully!")
        logger.info("=" * 80)
    else:
        logger.error("\n" + "=" * 80)
        logger.error("Conversion failed!")
        logger.error("=" * 80)


if __name__ == '__main__':
    main()

